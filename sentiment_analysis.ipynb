{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Forum Posts relating to Ukraine-Russia War"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_FILE = \"parsed_data.csv\"\n",
    "df = pd.read_csv(\"parsed_data.csv\")\n",
    "titles = df[\"title\"].values.astype(str)\n",
    "povs = df[\"pov\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(titles, pd.get_dummies(povs).values.astype(int), test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12358 unique tokens.\n",
      "Vocab size is 9589\n"
     ]
    }
   ],
   "source": [
    "VOCAB = set()\n",
    "for x in X_train:\n",
    "    VOCAB.add(x)\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "# Max number of words in title\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 60\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(f'Vocab size is {VOCAB_SIZE}')\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = 3\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=X_train.shape[1]),\n",
    "    tf.keras.layers.LSTM(512, dropout=0.5, recurrent_dropout=0.5, return_sequences=True, return_state=False),\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(units=512),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(units=256),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(units=128),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(units=64),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "\n",
    "    tf.keras.layers.Dense(units=32),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Dense(units=OUTPUT_SIZE),\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_metric = 'categorical_crossentropy'\n",
    "\n",
    "acc_metrics = [\n",
    "    'accuracy'\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "        loss=loss_metric, \n",
    "        metrics=acc_metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 264s 833ms/step - loss: 0.7996 - accuracy: 0.5602 - val_loss: 0.6700 - val_accuracy: 0.6533\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 257s 820ms/step - loss: 0.6415 - accuracy: 0.7164 - val_loss: 0.6056 - val_accuracy: 0.7005\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 265s 847ms/step - loss: 0.5235 - accuracy: 0.7920 - val_loss: 0.6471 - val_accuracy: 0.7077\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 264s 843ms/step - loss: 0.4223 - accuracy: 0.8368 - val_loss: 0.5935 - val_accuracy: 0.7149\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 266s 851ms/step - loss: 0.3439 - accuracy: 0.8704 - val_loss: 0.7081 - val_accuracy: 0.7189\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 269s 860ms/step - loss: 0.2835 - accuracy: 0.8949 - val_loss: 0.7494 - val_accuracy: 0.7229\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 264s 844ms/step - loss: 0.2456 - accuracy: 0.9150 - val_loss: 0.7530 - val_accuracy: 0.7225\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 261s 832ms/step - loss: 0.2056 - accuracy: 0.9283 - val_loss: 0.8212 - val_accuracy: 0.7197\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 253s 810ms/step - loss: 0.1772 - accuracy: 0.9340 - val_loss: 1.0398 - val_accuracy: 0.7165\n",
      "Epoch 9: early stopping\n",
      "Training Accuracy: 0.9640\n",
      "Testing Accuracy:  0.7165\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test), \n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          validation_split=0.1, \n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, min_delta=0.0001, verbose=1)])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-S23 (3.10)",
   "language": "python",
   "name": "csci1470"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
